---
title: "Statistical Learning Assignment 1"
author: "Tarun Sudhams"
date: "22nd October 2018"
output: html_notebook
---

## Question 1
*In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(Your UID) prior to starting part (a) to ensure consistent results.*

*(a) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0,1) distribution. This represents a feature, X.*
```{r}
set.seed(3876)
x=rnorm(100, mean = 0, sd = 1)
```
<br/>
*(b) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution i.e. a normal distribution with mean zero and variance 0.25.*

```{r}
eps = rnorm(100,mean=0,sd = 0.25)
```
<br/>
*(c) Using x and eps, generate a vector y according to the model Y = -1+0.5X+ε.*
```{r}
y = -1+0.5*x + eps
length(y)
```
The length of Y is 100 and value of β^o is -1 and β^1 is 0.5. <br/>
<br/>
*(d) Create a scatterplot displaying the relationship between x and y. Comment on what you observe.*
```{r}
plot(y~x)
```
The graph ploted above shows a linear relationship between Y and X. <br/>
<br/>
*(e) Fit a least squares linear model to predict y using x. Comment on the model obtained. How do βˆ0 and ?βˆ1 compare to β^o and β^1 ?*
```{r}
lm.fit = lm(y~x)
summary(lm.fit)
```
The linear regression fits very closely to the true value of the coefficients it was constructed from. The model has a large F-Statistic value and with a near zero p-value.
<br/>
*(f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.*
```{r}
plot(x,y)
abline(lm.fit,lwd=3,col=2)
abline(-1,0.5,lwd=3,col=3)
legend(-1, legend = c("Model Fit", "Pop Reg"), col=2:3,lwd=3)
```
<br/>
*(g) Now fit a polynomial regression model that predicts y using x and x 2 . Is there evidence that the quadratic term improves the model fit? Explain your answer.*

```{r}
lm.fit_sq = lm(y~x + I(x^2))
summary(lm.fit_sq)
```
<br/>
*(h) Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (1) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term ε in (b). Describe your results.*
```{r}
set.seed(3876)
eps1 = rnorm(100, 0, 0.125)
x1 = rnorm(100)
y1 = -1 + .5*x1 + eps1
plot(x1,y1)
lm.fit1 = lm(y1~x1)
summary(lm.fit1)

abline(lm.fit1, lwd=3,col=2)
abline(-1, 0.5, lwd=3,col=3)
legend(-1, legend = c("Model Fit", "Pop Reg"), col=2:3, lwd=3)
```
<br/>

*(i) Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (1) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term ε in (b). Describe your results.*
```{r}
set.seed(3876)
eps2 = rnorm(100,0,0.5)
x2 = rnorm(100)
y2 = -1 + 0.5*x2 + eps2
plot(x2,y2)
lm.fit2 = lm(y2~x2)
summary(lm.fit2)

abline(lm.fit2, lwd = 3, col = 2)
abline(-1, 0.5, lwd = 3, col = 3)
legend(-1, legend = c("Model Fit", "Pop reg"), col=2:3, lwd = 3)
```
On comparing the previous vales of R^2 and RSE, we can R^2 and RSE have increased by a considerable amount.
<br/>
*(j)What are the confidence intervals for β 0 and β 1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.*
```{r}
confint(lm.fit)
confint(lm.fit1)
confint(lm.fit2)
```
Seems like all the confidence intervals are approximately around the 0.5 mark. fit1's interval is observed to be slightly narrow than fit's interval and fit2's interval is wider than fit's interval. <br/>
<br/>

## Question 2
<br/>
*In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.*

*(a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.*
```{r}
library(ISLR)
summary(Weekly)
```
```{r}
data("Auto")
mpg01 <- rep(0,length(Auto$mpg))
mpg01[Auto$mpg > median(Auto$mpg)] <- 1
Auto <- data.frame(Auto, mpg01)
summary(Auto)
```
<br/>
*(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings. *
```{r}
cor(Auto[,-9])
```
<br/>

### Corrplot
```{r}
library(corrplot)
corrplot::corrplot.mixed(cor(Auto[,-9]), upper="circle")
```
### Scatterplot Matrix
```{r}
pairs(Auto[, -9])
```

### Boxplots
```{r}
par(mfrow=c(2,3))
boxplot(displacement ~ mpg01, data = Auto, main = "Displacement vs. mpg01")
boxplot(cylinders ~ mpg01, data = Auto, main = "Cylinders vs. mpg01")
boxplot(weight ~ mpg01, data = Auto, main = "Weight vs. mpg01")
boxplot(horsepower ~ mpg01, data = Auto, main = "Horsepower vs. mpg01")
boxplot(acceleration ~ mpg01, data = Auto, main = "Acceleration vs. mpg01")
boxplot(year ~ mpg01, data = Auto, main = "Year vs. mpg01")
```
There exists a clear anti-correlation between "mpg01" and cyclinders", "displacement", "horsepower" and "weight". <br/>
<br/>

*(c) Split the data into a training set and a test set.*
```{r}
set.seed(123)
train <- sample(1:dim(Auto)[1], dim(Auto)[1]*.7, rep=FALSE)
test <- train
training_data = Auto[train, ]
testing_data = Auto[test, ]
mpg01.test <- mpg01[test]
```
<br/>

*(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?*
```{r}
glm.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
    family = binomial, subset = train)
glm.probs = predict(glm.fit, testing_data, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```
The above logistic regression model has a 9.48% test error rate.<br/>
<br/>

## Question 3
<br/>
*We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.*

*(a) Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.*
```{r}
set.seed(3876)
attach(Default)
glm.fit = glm(default ~ income + balance, data = Default, family = binomial)
summary(glm.fit)
```
 The glm() estimates of standard errors for the coefficients β0, β1 and β2 are 0.434756, 4.9841572 x 10^{-6} and 2.2737314 x 10^{-4}<br/>
<br/>

*(b) Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.*
```{r}
boot.fn <- function(data,index)
{
  fit <-glm(default ~ income + balance, data = data, family = "binomial", subset = index)
  return (coef(fit))
}
```
<br/>

*(c) Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.*
```{r}
library(boot)
boot(Default, boot.fn, 1000)
```

The bootstrap estimates of the standard errors for the coefficients β0,β1 and β2 are 0.4239, 4.583 x 10^(-6) and 2.268 x 10^(-4) <br/>
<br/>

*(d) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function. *

The glm() estimates and bootstrap estimates of the standard errors are very similar to each other. 

## Question 4
<br/>
*We will now consider the Boston housing data set, from the MASS library.*

*(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate μˆ.  *
```{r}
library(MASS)
summary(Boston)
```
```{r}
set.seed(3876)
attach(Boston)
mu_hat <- mean(medv)
mu_hat
```
*(b) Provide an estimate of the standard error of μˆ. Interpret this result. *
```{r}
x.hat <- sd(medv) / sqrt(dim(Boston)[1])
x.hat
```
<br/>

*(c) Now estimate the standard error of μˆ using the bootstrap. How does this compare to your answer from (b)? *
```{r}
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bstrap = boot(medv, boot.fn, 1000)
bstrap
```

The bootsrap estimated standard error of μ̂of 0.40881 is very close to the estimate found in (b) of 0.4114. <br/>
<br/>

*(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv).*
```{r}
t.test(medv)
ConfidenceInterval.mu.hat <- c(22.53 - 2 * 0.4114, 22.53 + 2 * 0.4114)
ConfidenceInterval.mu.hat
```
The confidence interval of bootstrap is very close to the one returned test function. <br/>

*(e) Based on this dataset, provide an estimate, μˆ med , for the median value of medv in the population. *
```{r}
med_hat <- median(medv)
med_hat
```
<br/>

*(f) We now would like to estimate the standard error of μˆ med . Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings. *
```{r}
boot.fn = function(data,index) return (median(data[index]))
boot(medv, boot.fn, 1000)
```
The estimated median value of 21.2 is exactly the same as what was obtained in (e) with a standard error of 0.3927222 which is still quite small when compared to the value of the median.<br/>
<br/>

*(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston suburbs. Call this quantity μˆ 0.1 .*
```{r}
percentile_hat <- quantile(medv, c(0.1))
percentile_hat
```

*(h) Use the bootstrap to estimate the standard error of μ̂ 0.1 Comment on your findings.*
```{r}
boot.fn = function(data, index) return(quantile(data[index], c(0.1)))
boot(medv, boot.fn, 1000)
```
The estimated 10th percentile value of 12.75 is the same to the value obtained in (g) with a standard error of 0.5168 which is quite small when compared to the percentile value.